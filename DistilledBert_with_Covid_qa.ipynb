{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinh-thang/COS30018-Project-C/blob/main/Falcon_1b_with_Biomed_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full-finetuning Distilled Bert on Covid_qa_deepset dataset**"
      ],
      "metadata": {
        "id": "3vRu68obbKTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and install necessary libraries and dependancies"
      ],
      "metadata": {
        "id": "2d9tJUvjbrlF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulQOrFJtttqY"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops\n",
        "!pip install transformers[torch]\n",
        "!pip install datasets wandb\n",
        "!pip install --upgrade transformers\n",
        "!pip install evaluate numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqNNsge70-tV"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by loading the Covid_qa_deepset dataset from Huggingface. 70% of the dataset is extracted for training. 303 rows (not overlap with the training set) will be used for validation set"
      ],
      "metadata": {
        "id": "S_vNPiGxdME4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET1Dprijt3U3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"covid_qa_deepset\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset[\"train\"].train_test_split(train_size=1413,test_size=303)\n",
        "\n",
        "dataset = dataset.filter(lambda example: not \"answer_category\" in example['answers'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the tokenizer from the model"
      ],
      "metadata": {
        "id": "DFppnXv6dqqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# load distilled bert tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")"
      ],
      "metadata": {
        "id": "_BXoxdcPx5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the dataset based on the guideline provided by Huggingface"
      ],
      "metadata": {
        "id": "7aMAElrVdwxU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGBy_IQKugmz"
      },
      "outputs": [],
      "source": [
        "def preprocess_training(example):\n",
        "    inputs = tokenizer(\n",
        "        example['question'],\n",
        "        example[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = example[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "def preprocess_eval(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs\n",
        "\n",
        "training_dataset = dataset['train']\n",
        "training_dataset = training_dataset.map(preprocess_training, batched=True, remove_columns=dataset['train'].column_names)\n",
        "\n",
        "eval_dataset = dataset['test']\n",
        "eval_dataset = eval_dataset.map(preprocess_training, batched=True, remove_columns=dataset['test'].column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last 50 rows of the dataset will be used for testing, which is unseen by model. This is to give unbiased evaluation for the model's performance"
      ],
      "metadata": {
        "id": "1vYWyV3zehuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = load_dataset(dataset_name, split=\"train[1969:]\") #test dataset is extracted from last 50 rows of CovidQA dataset"
      ],
      "metadata": {
        "id": "w3W63Nng9da7"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the format of datasets after preprocess\n",
        "\n"
      ],
      "metadata": {
        "id": "mIEI7no_YPO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZgjQUowNVHU",
        "outputId": "4f7862b2-df8e-42e2-f736-06fca0e49605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 1413\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "training_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-fEGDKEOjpu",
        "outputId": "7fe81f26-4d7e-434e-ba75-08270f347fa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 303\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "eval_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3v-_CfiAhJH",
        "outputId": "a12b66bd-a8c8-4735-bd16-c266fa9b5f1d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBm1ECvC1vJs"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used is a variant of Bert, with 40% less parameters than typical Bert. This model is also finetuned with SQuAD dataset before."
      ],
      "metadata": {
        "id": "HJp4y-NRe1Vg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlSRQkO-uCF1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "\n",
        "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "\n",
        "\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\n",
        "    model_name,\n",
        ")\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T487AA26Kur"
      },
      "source": [
        "# Load evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load evaluation metric for the validation set. It can be either accuracy or F1 score"
      ],
      "metadata": {
        "id": "-pzUmqHEfrZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "85fr6fI26K2E"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load(\"f1\")\n",
        "#metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions[0], references=labels[0], average=\"macro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the training"
      ],
      "metadata": {
        "id": "FGRufu2OgGJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up data collator for data enumeration"
      ],
      "metadata": {
        "id": "D-V4flxlgLO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ],
      "metadata": {
        "id": "5OknQe_ZUpM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the training arguments"
      ],
      "metadata": {
        "id": "Sl_G0FvafylV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc4L3O5pwPLb"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"Distilled_bert_CovidQA_model\",\n",
        "    evaluation_strategy = \"steps\",\n",
        "    save_steps = 10,\n",
        "    logging_steps = 10,\n",
        "    max_steps = 500,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oJS7fDg-KnUw"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=training_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator = data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log in to wandb to configure training loss visualisation and save the monitor"
      ],
      "metadata": {
        "id": "2Qh9N9REgb5q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQd7XA8bEAnf"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"2ba90e109c0da4331467290174e54e3483f0d494\")\n",
        "wandb.init(project=\"QLoRA covidQA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1cwcKm0bgln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd9171cf-6101-43ad-9a74-a2f1bc13ef86"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 2:40:48, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.126500</td>\n",
              "      <td>0.906048</td>\n",
              "      <td>0.801980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.609300</td>\n",
              "      <td>0.718573</td>\n",
              "      <td>0.841584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.658100</td>\n",
              "      <td>0.718396</td>\n",
              "      <td>0.838284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.871600</td>\n",
              "      <td>0.726818</td>\n",
              "      <td>0.828383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.819900</td>\n",
              "      <td>0.686123</td>\n",
              "      <td>0.864686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.920200</td>\n",
              "      <td>0.678616</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.664600</td>\n",
              "      <td>0.650183</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.549400</td>\n",
              "      <td>0.663274</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.763000</td>\n",
              "      <td>0.688111</td>\n",
              "      <td>0.844884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.509100</td>\n",
              "      <td>0.664566</td>\n",
              "      <td>0.854785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.355000</td>\n",
              "      <td>0.683564</td>\n",
              "      <td>0.844884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.498100</td>\n",
              "      <td>0.695658</td>\n",
              "      <td>0.834983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.675090</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.484000</td>\n",
              "      <td>0.671820</td>\n",
              "      <td>0.844884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.517400</td>\n",
              "      <td>0.682905</td>\n",
              "      <td>0.841584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.434100</td>\n",
              "      <td>0.676078</td>\n",
              "      <td>0.844884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.463500</td>\n",
              "      <td>0.670002</td>\n",
              "      <td>0.858086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.441000</td>\n",
              "      <td>0.704178</td>\n",
              "      <td>0.831683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.541200</td>\n",
              "      <td>0.685012</td>\n",
              "      <td>0.861386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.255800</td>\n",
              "      <td>0.721505</td>\n",
              "      <td>0.838284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.297900</td>\n",
              "      <td>0.770630</td>\n",
              "      <td>0.854785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.306400</td>\n",
              "      <td>0.783976</td>\n",
              "      <td>0.834983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.306300</td>\n",
              "      <td>0.816872</td>\n",
              "      <td>0.821782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.439300</td>\n",
              "      <td>0.809790</td>\n",
              "      <td>0.841584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.415700</td>\n",
              "      <td>0.781189</td>\n",
              "      <td>0.861386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.306700</td>\n",
              "      <td>0.761300</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.763421</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.153300</td>\n",
              "      <td>0.802005</td>\n",
              "      <td>0.861386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.199900</td>\n",
              "      <td>0.809175</td>\n",
              "      <td>0.844884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.198400</td>\n",
              "      <td>0.818069</td>\n",
              "      <td>0.841584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.183600</td>\n",
              "      <td>0.839833</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.337400</td>\n",
              "      <td>0.840909</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.855369</td>\n",
              "      <td>0.821782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.391400</td>\n",
              "      <td>0.809833</td>\n",
              "      <td>0.838284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.357900</td>\n",
              "      <td>0.782707</td>\n",
              "      <td>0.834983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.303200</td>\n",
              "      <td>0.770879</td>\n",
              "      <td>0.834983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.190500</td>\n",
              "      <td>0.778935</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.115600</td>\n",
              "      <td>0.792268</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.239000</td>\n",
              "      <td>0.796339</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.217000</td>\n",
              "      <td>0.812068</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.168900</td>\n",
              "      <td>0.817628</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.153100</td>\n",
              "      <td>0.823335</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.159000</td>\n",
              "      <td>0.832959</td>\n",
              "      <td>0.854785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.150300</td>\n",
              "      <td>0.835691</td>\n",
              "      <td>0.854785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.175600</td>\n",
              "      <td>0.837952</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.152600</td>\n",
              "      <td>0.839607</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.839053</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.186600</td>\n",
              "      <td>0.840549</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.141600</td>\n",
              "      <td>0.843356</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.843828</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.38069887399673463, metrics={'train_runtime': 9669.35, 'train_samples_per_second': 0.827, 'train_steps_per_second': 0.052, 'total_flos': 1038038879754240.0, 'train_loss': 0.38069887399673463, 'epoch': 5.62})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AcFLlV_Ba8P"
      },
      "source": [
        "#Run evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is evaluation on validation dataset, which is somewhat biased after finetuning as it has already been seen by model"
      ],
      "metadata": {
        "id": "xR5fOIn5gyYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "QPPs2B8i4Coa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "e722acaa-2eee-4c37-9cfb-59de35853e01"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19/19 5:55:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.8438284397125244,\n",
              " 'eval_f1': 0.294705621956618,\n",
              " 'eval_runtime': 96.3285,\n",
              " 'eval_samples_per_second': 3.145,\n",
              " 'eval_steps_per_second': 0.197}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "#evaluation with validation dataset\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is evaluation on test dataset, which has not been seen by the model"
      ],
      "metadata": {
        "id": "xC43PPrrhF8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation with test dataset (unseen by model)\n",
        "from transformers import QuestionAnsweringPipeline\n",
        "\n",
        "pipeline = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
        "\n",
        "total_f1 = 0\n",
        "total_accuracy = 0\n",
        "\n",
        "for ins in test_dataset:\n",
        "  ans = pipeline(question=ins['question'], context=ins['context'], max_answer_len=50, max_question_len=300)\n",
        "  ref_tokens = tokenizer(\" \" + ins[\"answers\"][\"text\"][0])[\"input_ids\"]\n",
        "  ans_tokens = tokenizer(ans[\"answer\"])[\"input_ids\"]\n",
        "  common_tokens = set(ans_tokens) & set(ref_tokens)\n",
        "  precision = len(common_tokens) / len(ans_tokens)\n",
        "  recall = len(common_tokens) / len(ref_tokens)\n",
        "  total_accuracy += precision\n",
        "  print(tokenizer.decode(ans_tokens), \"|\", tokenizer.decode(ref_tokens), \"|\")\n",
        "  if (len(common_tokens) == 0):\n",
        "    total_f1 += 0\n",
        "    print(0)\n",
        "  else:\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    total_f1 += f1\n",
        "    print(f1)"
      ],
      "metadata": {
        "id": "rstWlRr_K26H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"F1 average score:\", total_f1 / 50)\n",
        "print(\"Accuracy average score: \", total_accuracy / 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvHHHSHGLyDg",
        "outputId": "a056ecd6-5004-4316-f36e-b9ed8a931ffd"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 average score: 0.633776704091313\n",
            "Accuracy average score:  0.7655031439244173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px-PfCWUwbTl"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test inference for the model before and after finetuning"
      ],
      "metadata": {
        "id": "7UNAJVI_hBFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training\n",
        "from transformers import pipeline, QuestionAnsweringPipeline\n",
        "\n",
        "question = \"Where did Covid-19 originated\"\n",
        "context = 'According to epidemiological studies, the Huanan Market in Wuhan was the early and main epicentre of SARS‐CoV‐2 infection'\n",
        "print(question)\n",
        "\n",
        "pipe = pipeline(model=\"distilbert-base-uncased-distilled-squad\")\n",
        "pipe(question=question, context=context, max_answer_len=300, max_question_len=300)"
      ],
      "metadata": {
        "id": "3p8mdvkmgxCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd43a8eb-6115-4bc0-b544-c68de1fb4724"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Where did Covid-19 originated\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.35626348853111267,\n",
              " 'start': 42,\n",
              " 'end': 64,\n",
              " 'answer': 'Huanan Market in Wuhan'}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CAeq2AM9atEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884e35de-c936-4c26-a16e-84af0fc77020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Where did Covid-19 originated\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.4766525328159332,\n",
              " 'start': 42,\n",
              " 'end': 64,\n",
              " 'answer': 'Huanan Market in Wuhan'}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "#After training\n",
        "from transformers import QuestionAnsweringPipeline\n",
        "question = \"Where did Covid-19 originated\"\n",
        "context = \"According to epidemiological studies, the Huanan Market in Wuhan was the early and main epicentre of SARS‐CoV‐2 infection\"\n",
        "print(question)\n",
        "#print(context)\n",
        "\n",
        "pipe2 = QuestionAnsweringPipeline(model=model, tokenizer = tokenizer)\n",
        "pipe2(question=question, context=context, max_answer_len=300, max_question_len=300)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}