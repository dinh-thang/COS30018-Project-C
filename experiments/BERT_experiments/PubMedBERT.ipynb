{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBH+s+Fn7xtdNPAVv+/S9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinh-thang/COS30018-Project-C/blob/main/PubMedBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUGt9OLk8VMn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "GVyDXf42giBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "4gPpLlinitqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertModel, BertModel\n",
        "from transformers import DistilBertTokenizer, BertTokenizerFast, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import default_data_collator\n"
      ],
      "metadata": {
        "id": "8VgPutN89JlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"covid_qa_deepset\"\n",
        "\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "metadata": {
        "id": "vdNmdNPu-r21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "0RMS2pli_mRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "tokenizer_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)"
      ],
      "metadata": {
        "id": "T5GmzvcqSA9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "\n",
        "def preprocess_training_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label is (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "fWwvLRaPfVCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset[\"train\"].map(preprocess_training_examples, batched=True, remove_columns=dataset[\"train\"].column_names)\n"
      ],
      "metadata": {
        "id": "v9YxgS40NsIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = dataset[\"test\"].map(preprocess_training_examples, batched=True, remove_columns=dataset[\"test\"].column_names)\n"
      ],
      "metadata": {
        "id": "uMyuIpwnf32B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "2Co911cnhgQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "mbTFo7lmiFTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"qthang-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "YyRynUz5hwzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = default_data_collator\n"
      ],
      "metadata": {
        "id": "7jSfHqS9iRed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=training_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "NxemmiNnig84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA1Cw8Wsg8Pm",
        "outputId": "5057bf47-6af8-4e06-91b4-49a97037ead0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 11617\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import evaluator\n",
        "from transformers import AutoModelForCausalLM, BertForQuestionAnswering, TFAutoModelForQuestionAnswering\n",
        "\n",
        "eval_model = BertForQuestionAnswering.from_pretrained(\n",
        "    \"ThangDinh/qthang-finetuned\",\n",
        "    trust_remote_code=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "guc9w6b5eXrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "AXpdUU1SmTHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "MqSKdUzKYdwv",
        "outputId": "a5cc2841-5953-4402-cb24-2b91e9926d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='727' max='727' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [727/727 05:18]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.2763604521751404,\n",
              " 'eval_runtime': 319.0849,\n",
              " 'eval_samples_per_second': 36.407,\n",
              " 'eval_steps_per_second': 2.278,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"covid_qa_deepset\"\n",
        "\n",
        "test_dataset = load_dataset(dataset_name, split=\"train[:10]\")"
      ],
      "metadata": {
        "id": "8tusO3wgMJI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import QuestionAnsweringPipeline\n",
        "\n",
        "pipeline = QuestionAnsweringPipeline(model=eval_model, tokenizer=tokenizer)\n",
        "\n",
        "total_f1 = 0\n",
        "total_accuracy = 0\n",
        "\n",
        "for ins in test_dataset:\n",
        "  ans = pipeline(question=ins['question'], context=ins['context'], max_answer_len=50, max_question_len=300)\n",
        "  ref_tokens = tokenizer(\" \" + ins[\"answers\"][\"text\"][0])[\"input_ids\"]\n",
        "  ans_tokens = tokenizer(ans[\"answer\"])[\"input_ids\"]\n",
        "  common_tokens = set(ans_tokens) & set(ref_tokens)\n",
        "  precision = len(common_tokens) / len(ans_tokens)\n",
        "  recall = len(common_tokens) / len(ref_tokens)\n",
        "  total_accuracy += precision\n",
        "  print(tokenizer.decode(ans_tokens), \"|\", tokenizer.decode(ref_tokens), \"|\")\n",
        "  if (len(common_tokens) == 0):\n",
        "    total_f1 += 0\n",
        "    print(0)\n",
        "  else:\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    total_f1 += f1\n",
        "    print(f1)\n",
        "\n",
        "print(\"F1 average score:\", total_f1 / 100)\n",
        "print(\"Accuracy average score: \", total_accuracy / 100)\n",
        "\n"
      ],
      "metadata": {
        "id": "8QaIF5LuVVE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "from transformers import QuestionAnsweringPipeline\n",
        "\n",
        "pipeline = QuestionAnsweringPipeline(model=eval_model, tokenizer=tokenizer)\n",
        "\n",
        "bleu = evaluate.load(\"f1\")\n",
        "\n",
        "for example in test_dataset:\n",
        "    question = [example[\"question\"]]\n",
        "    context = [example[\"context\"]]\n",
        "    references = example[\"answers\"][\"text\"][0]\n",
        "\n",
        "    model_predictions = pipeline(question=question, context=context, max_answer_len=50, max_question_len=300)\n",
        "    bleu.add_batch(predictions=[model_predictions[\"answer\"]], references=[references])\n",
        "final_score = bleu.compute()"
      ],
      "metadata": {
        "id": "EesTUZaOfLr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_scoreA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x-NQ_Ri4NNi",
        "outputId": "3a51b8f2-0f54-4543-9274-ace684445a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 0.5}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question #original question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pQzShXeC_-Ov",
        "outputId": "22511d72-b599-4393-a6b1-ac40644168e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is the effect of Mannanose Binding Lectin in the elimination of HIV-1 pathogen? \"\n",
        "context = test_dataset[9][\"context\"]\n",
        "answer = test_dataset[9][\"answers\"][\"text\"][0]\n",
        "\n",
        "result = pipeline(question=question, context=context, max_answer_len=100, max_question_len=300)\n",
        "\n",
        "print(\"predicted:\" + result[\"answer\"])\n",
        "print(\"reference:\" + answer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFRVPB629x3o",
        "outputId": "7bb9be3d-deb3-488d-e2fc-206390871c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted:the C-type lectin receptor, dendritic cell-specific ICAM-grabbing non-integrin-related (DC-SIGNR, also known as CD209L or liver/lymph node–specific ICAM-grabbing non-integrin (L-SIGN)), can interact with pathogens including HIV-1 and is expressed at the maternal-fetal interface, we hypothesized that it could influence MTCT of HIV-1\n",
            "reference:Mannose-binding lectin (MBL) is an innate immune receptor synthesised in the liver and secreted in the bloodstream in response to inflammation signal. MBL promotes pathogen elimination by opsonization and phagocytosis,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2HMy2EJ-YZl",
        "outputId": "eafea201-c3bb-4527-bba3-809078c2c216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 3.246519997279762e-10, 'start': 477, 'end': 810, 'answer': 'the C-type lectin receptor, dendritic cell-specific ICAM-grabbing non-integrin-related (DC-SIGNR, also known as CD209L or liver/lymph node–specific ICAM-grabbing non-integrin (L-SIGN)), can interact with pathogens including HIV-1 and is expressed at the maternal-fetal interface, we hypothesized that it could influence MTCT of HIV-1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "f1_score = f1.compute(predictions=[result[\"answer\"]], references=[test_dataset[9][\"answers\"][\"text\"][0]])\n",
        "\n",
        "print(f1_score)\n"
      ],
      "metadata": {
        "id": "gmr7TY49Br75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"glue\")\n",
        "\n",
        "for example in test_dataset:\n",
        "    question = [example[\"question\"]]\n",
        "    context = [example[\"context\"]]\n",
        "    references = example[\"answers\"][\"text\"][0]\n",
        "\n",
        "    model_predictions = pipeline(question=question, context=context, max_answer_len=50, max_question_len=300)\n",
        "    bleu.add_batch(predictions=[model_predictions[\"answer\"]], references=[references])\n",
        "final_score = bleu.compute()\n"
      ],
      "metadata": {
        "id": "833GltVVls7z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}