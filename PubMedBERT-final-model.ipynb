{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5fZobiih3FK3ov/9DpnV4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Full finetuning PubMedBERT with Covid_qa_deepset dataset**\n",
        ">Author: Quang Thang Dinh - Team 4 | Option C project\n"
      ],
      "metadata": {
        "id": "Dg8KO7MSXD-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install neccessary libraries and dependencies"
      ],
      "metadata": {
        "id": "uaLylAfGYTVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUGt9OLk8VMn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install evaluate\n",
        "!pip install accelerate\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, BertTokenizerFast, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "from transformers import DistilBertModel, BertModel\n",
        "from transformers import QuestionAnsweringPipeline\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import default_data_collator\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from evaluate import evaluator\n",
        "from evaluate import load\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch"
      ],
      "metadata": {
        "id": "8VgPutN89JlV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model I'm going to use is PubMedBERT from Microsoft https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext.\n",
        "\n",
        "The dataset used for finetuning is the covide_qa_deepset, with 50 last examples dedicated for evaluating. Therefore, I have a total of 1969 examples for training with a train_test_split of 0.2"
      ],
      "metadata": {
        "id": "qxz1HSagYYxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# general constants\n",
        "dataset_name = \"covid_qa_deepset\"\n",
        "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "tokenizer_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "\n",
        "# loading the dataset and tokenizer\n",
        "dataset = load_dataset(dataset_name, split=\"train[:1969]\")\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)"
      ],
      "metadata": {
        "id": "vdNmdNPu-r21"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model for question answering task"
      ],
      "metadata": {
        "id": "qL--BgBMVKr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2Co911cnhgQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess function to format the input dataset. The detail explanation of this process will be covered in the team's report"
      ],
      "metadata": {
        "id": "cpL0aXoXZhAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocess function\n",
        "def preprocess_data(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=256,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "fWwvLRaPfVCO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset[\"train\"].map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "eval_dataset = dataset[\"test\"].map(preprocess_data, batched=True, remove_columns=dataset[\"test\"].column_names)"
      ],
      "metadata": {
        "id": "v9YxgS40NsIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessed, the dataset will have the format"
      ],
      "metadata": {
        "id": "8JQsiKLOae_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHbHKD4lajun",
        "outputId": "b44e8abe-7526-4b96-f3b2-d3fb90c0d32d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 43578\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the training arguments and loading the data collator"
      ],
      "metadata": {
        "id": "G9MjdELZatYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"qthang-finetuned\"\n",
        "evaluation_strategy = \"epoch\"\n",
        "learning_rate = 2e-5\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "num_train_epochs = 3\n",
        "weight_decay = 0.01\n",
        "push_to_hub = False\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    evaluation_strategy=evaluation_strategy,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    weight_decay=weight_decay,\n",
        "    push_to_hub=push_to_hub,\n",
        ")\n",
        "\n",
        "data_collator = default_data_collator\n"
      ],
      "metadata": {
        "id": "YyRynUz5hwzm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=training_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "NxemmiNnig84"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "AXpdUU1SmTHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the finetuned model and dataset for evaluation"
      ],
      "metadata": {
        "id": "numCUM_faz-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_finetuned = \"ThangDinh/qthang-finetuned\"\n",
        "\n",
        "eval_model = BertForQuestionAnswering.from_pretrained(\n",
        "    model_finetuned,\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "guc9w6b5eXrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"covid_qa_deepset\"\n",
        "\n",
        "test_dataset = load_dataset(dataset_name, split=\"train[1969:]\").shuffle()"
      ],
      "metadata": {
        "id": "8tusO3wgMJI-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the BLEU and exact match score of the model. To calculate exact match, replace the metric string with \"exact-match\"."
      ],
      "metadata": {
        "id": "opWTPtxHbf-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load(\"bleu\")\n",
        "\n",
        "for example in test_dataset:\n",
        "    question = [example[\"question\"]]\n",
        "    context = [example[\"context\"]]\n",
        "    references = example[\"answers\"][\"text\"][0]\n",
        "\n",
        "    model_predictions = pipeline(question=question, context=context, max_answer_len=50, max_question_len=300)\n",
        "    metric.add_batch(predictions=[model_predictions[\"answer\"]], references=[references])\n",
        "final_score = metric.compute()\n"
      ],
      "metadata": {
        "id": "833GltVVls7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate the f1 and accuracy score based on HuggingFace documentation https://huggingface.co/spaces/evaluate-metric/f1 by team member Quoc Bao Pham."
      ],
      "metadata": {
        "id": "OhCyrQrTbHZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = QuestionAnsweringPipeline(model=eval_model, tokenizer=tokenizer)\n",
        "\n",
        "total_f1 = 0\n",
        "total_accuracy = 0\n",
        "\n",
        "for ins in test_dataset:\n",
        "  ans = pipeline(question=ins['question'], context=ins['context'], max_answer_len=50, max_question_len=300)\n",
        "  ref_tokens = tokenizer(\" \" + ins[\"answers\"][\"text\"][0])[\"input_ids\"]\n",
        "  ans_tokens = tokenizer(ans[\"answer\"])[\"input_ids\"]\n",
        "  common_tokens = set(ans_tokens) & set(ref_tokens)\n",
        "  precision = len(common_tokens) / len(ans_tokens)\n",
        "  recall = len(common_tokens) / len(ref_tokens)\n",
        "  total_accuracy += precision\n",
        "  print(tokenizer.decode(ans_tokens), \"|\", tokenizer.decode(ref_tokens), \"|\")\n",
        "  if (len(common_tokens) == 0):\n",
        "    total_f1 += 0\n",
        "    print(0)\n",
        "  else:\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    total_f1 += f1\n",
        "    print(f1)\n",
        "\n",
        "print(\"F1 average score:\", total_f1 / 100)\n",
        "print(\"Accuracy average score: \", total_accuracy / 100)"
      ],
      "metadata": {
        "id": "8QaIF5LuVVE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, sample inferencing using the 9th example in the test dataset"
      ],
      "metadata": {
        "id": "wph7hJISb3bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = test_dataset[9][\"context\"]\n",
        "context = test_dataset[9][\"context\"]\n",
        "answer = test_dataset[9][\"answers\"][\"text\"][0]\n",
        "\n",
        "result = pipeline(question=question, context=context, max_answer_len=100, max_question_len=300)\n",
        "\n",
        "print(\"predicted:\" + result[\"answer\"])\n",
        "print(\"reference:\" + answer)"
      ],
      "metadata": {
        "id": "rFRVPB629x3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}