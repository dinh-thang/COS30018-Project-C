{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVdUB2IiZyYL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbWjqFQA9P9n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2EgqEPDQ8v6"
      },
      "source": [
        "## Finetune Falcon-7b (sharded version) on a Google colab notebook\n",
        "\n",
        "Project C - Team 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-tTvEF1RT3y"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The used libraries are `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent `SFTTrainer`. We will use `bitsandbytes` to quantize the base model into 4bit (QLoRA approach). We will also install `einops` as it is a requirement to load Falcon models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNnkgBq7Q3EU",
        "outputId": "344fcc2e-2401-4b10-f35f-1ef8c0f8c3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnqmq7amRrU8"
      },
      "source": [
        "## Dataset load\n",
        "\n",
        "We make use of PubMedQA, which has more than 200000 instances (Artificially created)\n",
        "\n",
        "The dataset can be found [here](https://huggingface.co/datasets/pubmed_qa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av3p-4sq1C1H"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"pbaoo2705/processed_dataset_v2\"\n",
        "dataset = load_dataset(dataset_name, split='train')\n",
        "eval_dataset = load_dataset(dataset_name, split='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjOMoSbGSxx9"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjB0WAqFSzlD"
      },
      "source": [
        "In this section we will load the [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantize it in 4bit and attach LoRA adapters on it. Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "983992e01401488d9d350d4e9ec537d1",
            "a64542c3f1d74b4aa7a4760675daf6c9",
            "443daf1cd524444594c8a262da06ac63",
            "40554a4a7ce8402989b52f9cb26b86d4",
            "72b85fd9fe344586b3ed51608982aa29",
            "1c72e23a91d045778bf860643ccaabc7",
            "104eb64722e341cda6490b6fa93f367f",
            "8e628cc99bd2478184d0a49ce9f7bec4",
            "320ee4c338854ef4be4553e8434ac9a8",
            "69fee7f5f35e46eca80c42219ef66c79",
            "2e25a98b51fa46fbb679cfe994e751f1"
          ]
        },
        "id": "ZwXZbQ2dSwzI",
        "outputId": "640a31b3-4a19-43f9-db63-851950163268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type RefinedWebModel to instantiate a model of type falcon. This is not supported for all configurations of models and can yield errors.\n",
            "WARNING:transformers_modules.tiiuae.falcon-7b.898df1396f35e447d5fe44e0a3ccaaaa69f30d36.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "983992e01401488d9d350d4e9ec537d1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import  AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
        "model_name = \"cosmin/falcon-7b-sharded-bf16\"\n",
        "\n",
        "#Quantizer initialize\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "#Get model from HuggingFace's transformers library\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDS2yYmlUAD6"
      },
      "outputs": [],
      "source": [
        "#Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuAx3zBeUL1q"
      },
      "source": [
        "Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `dense`, `dense_h_to_4_h` and `dense_4h_to_h` layers in the target modules in addition to the mixed query key value layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3xKJ4Etjrfk",
        "outputId": "b73bea84-6767-4acc-d847-ea82461e4b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FalconForCausalLM(\n",
            "  (transformer): FalconModel(\n",
            "    (word_embeddings): Embedding(65024, 4544)\n",
            "    (h): ModuleList(\n",
            "      (0-31): 32 x FalconDecoderLayer(\n",
            "        (self_attention): FalconAttention(\n",
            "          (maybe_rotary): FalconRotaryEmbedding()\n",
            "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
            "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (mlp): FalconMLP(\n",
            "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
            "          (act): GELU(approximate='none')\n",
            "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQdvjTYTT1vQ"
      },
      "outputs": [],
      "source": [
        "from peft import PromptEncoderConfig\n",
        "\n",
        "#Use LoRA config for fine-tuning this model\n",
        "peft_config = PromptEncoderConfig(task_type=\"CAUSAL_LM\", num_virtual_tokens=20, encoder_hidden_size=128)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzsYHLwIZoLm"
      },
      "source": [
        "## Loading the trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTBJVE4PaJwK"
      },
      "source": [
        "Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCFTvGW6aspE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Adafactor\n",
        "\n",
        "#Arguments needed for training process\n",
        "output_dir = \"falcon-7b-sharded\"\n",
        "per_device_train_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "#Paged adamw 32 bits optimization algorithm is used in QLoRA\n",
        "optim = \"adafactor\"\n",
        "save_steps = 10\n",
        "logging_steps = 10\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "max_steps = 500\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    eval_accumulation_steps=1,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    report_to=None,\n",
        "    lr_scheduler_type=lr_scheduler_type\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNS-eonR2cNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21052c52-c9f4-4c34-9b5f-b05cdab221e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19468, 6823, 37, 1853, 299, 3131, 241, 1866, 9705, 544, 248, 23335, 275, 248, 1650, 204, 13, 3032, 387, 1799, 100, 13, 272, 17750, 42, 4012, 745, 6432, 3236, 271, 2879, 988, 387, 1799, 424, 272, 248, 17170, 1208, 273, 27421, 5228, 1959, 25, 19468, 12453, 37, 204, 13, 8350, 387, 1799, 100, 13, 11869, 271, 241, 1208, 4352, 881, 629, 304, 736, 532, 12639, 312, 241, 2057, 822, 379, 1506, 25, 529, 17750, 23, 414, 1650, 304, 4235, 5228, 272, 248, 5477, 1208, 23, 881, 241, 988, 387, 1799, 100, 10679, 504, 2589, 1484, 648, 248, 17324, 273, 1660, 3192, 275, 525, 3900, 25, 390, 5312, 275, 241, 988, 387, 1799, 100, 418, 1226, 272, 3089, 17324, 273, 6511, 8093, 4269, 312, 4862, 23, 345, 248, 10679, 504, 1278, 19785, 271, 2638, 17324, 379, 1586, 1286, 1660, 3192, 25, 193, 193, 15770, 1959, 504, 6935, 2879, 988, 387, 1799, 424, 272, 9462, 963, 345, 5543, 273, 2794, 1655, 23, 881, 241, 1218, 1902, 2404, 1873, 241, 2589, 8091, 275, 248, 1208, 204, 19, 45, 419, 594, 204, 17, 22399, 2776, 23, 204, 626, 30, 691, 529, 783, 9462, 23, 4862, 1764, 1936, 1958, 17324, 23, 3991, 3194, 23, 273, 6511, 38659, 1484, 23, 3736, 271, 241, 3289, 881, 506, 362, 12028, 313, 248, 10679, 312, 525, 31474, 25, 735, 25990, 418, 1226, 272, 2515, 30627, 275, 17324, 273, 241, 11162, 272, 1660, 3192, 25, 193, 193, 18298, 23, 248, 3722, 275, 988, 387, 1799, 100, 304, 3978, 271, 3906, 248, 15719, 275, 5477, 5751, 273, 248, 2549, 275, 1208, 1484, 313, 4862, 25, 7716, 1959, 304, 2434, 271, 1576, 248, 8572, 273, 2549, 275, 988, 387, 1799, 424, 313, 248, 5245, 273, 271, 1128, 5966, 271, 2312, 414, 2645, 25, 193, 193, 20576, 37, 193, 45, 419, 594, 23, 462, 1702, 204, 17, 22399, 2776, 23, 393, 25, 204, 19, 626, 30, 691, 390, 5350, 275, 16114, 7910, 1119, 273, 8734, 29549, 345, 26306, 275, 382, 586, 272, 4818, 204, 28, 38791, 529, 6627, 25, 6158, 275, 12834, 49320, 23, 204, 2102, 19, 30, 770, 204, 5831, 24, 5070, 25, 19468, 6823, 37, 2784, 5656, 334, 271, 241, 3696]\n"
          ]
        }
      ],
      "source": [
        "#Get text example\n",
        "#print(dataset_train['text'][0])\n",
        "tokens = tokenizer.tokenize(\"### Human: Can you write a short introduction about the relevance of the term \\\"monopsony\\\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \\\"Monopsony\\\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\")\n",
        "print(tokenizer.convert_tokens_to_ids(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3t6b2TkcJwy"
      },
      "source": [
        "Then finally pass everthing to the trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl9K22OcrYQu",
        "outputId": "c63adde0-5984-4705-fdb6-e9ceadd006af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNeOBgZeTl2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63590964-5d7a-4462-b5df-4e8af5d6e83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:107: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 512\n",
        "eval_subset = eval_dataset.select(range(200))\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    eval_dataset=eval_subset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWplqqDjb3sS"
      },
      "source": [
        "We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw0nn3j9V7yq",
        "outputId": "f91363f7-323f-4430-9ddd-171f19c5352b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision', 'text'],\n",
            "    num_rows: 5000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OyIvEx7b1GT"
      },
      "outputs": [],
      "source": [
        "for name, module in trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JApkSrCcL3O"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjvisllacNZM"
      },
      "source": [
        "Now let's train the model! Simply call `trainer.train()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWhndoLl-9lR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef49d21-6f0c-4959-878b-56a0b3867288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhungnguyennguyen200504\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kbS7nRxcMt7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "dc517965-b746-4e7c-ca16-3a4ca31876ee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 11/500 00:37 < 33:33, 0.24 it/s, Epoch 0.03/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 03:58]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1982\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3067\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3357\u001b[0m                 )\n\u001b[1;32m   3358\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3359\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3360\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3361\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "g_Bl0dET_9FF",
        "outputId": "e1d57b74-ad29-4b08-ab54-4a1ba17d3734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 78/250 07:20 < 16:23, 0.17 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5c0ppfasK29"
      },
      "source": [
        "During training, the model should converge nicely as follows:\n",
        "\n",
        "![image](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/loss-falcon-7b.png)\n",
        "\n",
        "The `SFTTrainer` also takes care of properly saving only the adapters during training instead of saving the entire model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbAukwMF5lIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ae1480-7149-404b-b1d9-4690e53e0264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrKs-7UL5r6V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "fd91fc1658184370af5b0a6cc14d4edb",
            "30fa6a3286304c6c93ef89cd07694e3a",
            "a21541a391204c868bd68c5d3dfbe264",
            "bd1035a635424d09a6c8a1c7c20fc9c3",
            "e9379466b60344749c6fa49b2109ac14",
            "bb826fed12a24c2b975d7c69634ec4c3",
            "9878d283243f49ea83572e4b2c156447",
            "8439c96f08714c129d9c669f462eb826",
            "c7253359a80047879566e7795f2a2af8",
            "0cf3ef1038494471b2f61e9ad164c486",
            "d7ee6fe4a9af4af7a679e37f0b34a6b9",
            "913dd14776cb4061989eefe63ce5dfc7",
            "d10b4aeb5ea8452b91b209a5ff6afddd",
            "f81e18b09dc64716b0bfca931866de59",
            "4ca9ea34a09b46d39f3deafd6af6bd15",
            "b663f735c47f4a42bfbad406f04db686",
            "665d4d288efe454ebcee0a0f60f92f47",
            "ddddf4e062054ed09e97f4b6563d1426",
            "c9770ccd4ef34923af5016e7fc05d242",
            "b582305bf9074d21bf62298bd5495bcf",
            "906c8201d347416a92a8929cb9c558f4",
            "e212ab60e73c4f20bb5eab6b17768bcd",
            "e5389640abb14d2a8e7b8c9a13f967b2",
            "59e83d56182d48a9b5201a255a1f6516",
            "37899d1458dc44f2be95c39b659a8304",
            "bd2c0056ecf345dbab0595ef20a389f9",
            "ada7d39fcec14f5f85b9ea383b117ce8",
            "6b075d2748d842a6ba8321d5f20892ba",
            "64abe2105c4b45289a5fb5cfe09243e9",
            "924783bf8bd3407a8e49d41de244b71e",
            "dd8c22a883914791a58206565cf899d6",
            "1b3375bd603a4c9f8b5d70876aa57541"
          ]
        },
        "outputId": "7ddb9141-77b6-410c-bb8e-4de3bcabe1e9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd91fc1658184370af5b0a6cc14d4edb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Login to huggingface\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwSsElLr5_kP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "901acb6b6a5d464e96ae6aed66327544",
            "ddd7f0fbb2fe4556b64b409e25a5f16e",
            "8a54b2a5bd444424ab7965be50717183",
            "43e078ad001a4b34b0095f0ac986f7a0",
            "8802bd67473344d1a45d9c3da9b02a0b",
            "8e4b22a1262b4e63bfc685441854812d",
            "13b72658f2e441dc8eadb8a67bb10cd9",
            "53e73c7801e044ce9efabce5afae978c",
            "a5f323e469b64cfeb57c5edab10ab207",
            "f9df8d7a4ab74a93be6c0e99a88215b7",
            "e46bb5e05f0e4784ad727989bf9c8910",
            "44a88a7872274f2f8528f59b33b7aee4",
            "4d739fafe20a401d9ecfa3f8d4fd644a",
            "59681d4807c445259b4032bbfbfbb44a",
            "9556406e47f8457da4fedc3c0dcba70c",
            "d232ae0820df481f859e8e5f275763a1",
            "ad84856107644c58982de43da2097a09",
            "17740ec1b4e0458e9184c353658113e9",
            "6b88329c44964884af9abd663e80128f",
            "866b445b74f643f090c4cabe6b3a1d34",
            "f5ce39decc624f3b9cbfde242522da8c",
            "0e542e3d8b304b029a3d85302417bb17",
            "37aec9449f5d4a659ae118d5fa596f80",
            "8320ede10e6247bca745a758ec328f84",
            "82d3eb049edb40c7864f63c3722e3257",
            "4ced92478b2f4b97b1eb622206eada17",
            "049ca9c27a0b4bf8945c47f8104ef757",
            "296506bb1c4547dd838431047e67b1c7",
            "0fc3fa9cb6fb456a915b334ae13df5c1",
            "43a8c77014824cd0b64d73601aef47d6",
            "42f37127b97240e1ac7f6359517ed2e3",
            "9d63f8c4f7fd4819a49a4633e9a9e36e",
            "e64fb7aaaad34bcdb70a113bd0165bb4"
          ]
        },
        "outputId": "21b596a5-b74c-4249-e00d-e7db9cca4be7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "901acb6b6a5d464e96ae6aed66327544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/4.09k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44a88a7872274f2f8528f59b33b7aee4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.bin:   0%|          | 0.00/364k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37aec9449f5d4a659ae118d5fa596f80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/hung200504/falcon-7b-sharded/tree/main/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Push model to hub\n",
        "trainer.push_to_hub(\"falcon-7b-sharded-adafactor-constant-p-tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4PytmYo3Vx8"
      },
      "outputs": [],
      "source": []
    }
  ]
}